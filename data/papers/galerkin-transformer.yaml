# This file is the source-of-truth for the paper page on the website.
# Edit fields to curate experiments/baselines/results.
slug: galerkin-transformer
short_title: Galerkin Transformer
full_title: 'Choose a Transformer: Fourier or Galerkin'
authors: Shanghong Cao
year: 2021
category: Transformers for PDEs
badges:
- Transformer
- Operator learning
tagline: Operator learning with attention variants tied to numerical discretizations.
quick_facts:
- 'Type: attention-based operator'
- Fourier/Galerkin attention
- Mesh-independent flavor
links:
  paper: https://arxiv.org/abs/2105.14995
  paper_label: arXiv:2105.14995
tldr: This work adapts transformer attention to operator learning with Fourier or Galerkin-inspired variants, aiming for efficient and stable PDE surrogate modeling.
contrib:
- Introduces Fourier and Galerkin attention mechanisms inspired by numerical solvers.
- Demonstrates operator learning on benchmark PDEs.
- Shows efficiency advantages over naive attention for high-resolution fields.
pdes:
- Burgers
- Darcy flow
- Inverse interface coefficient problem
tasks:
- Forward operator learning
- Inverse problem inference
baselines:
- FNO
- DeepONet
- U-Net
setting:
- Supervised operator learning; compares attention variants.
- Evaluates on standard PDE datasets and inverse coefficient estimation.
results_tables:
- title: Key results
  header:
  - Benchmark
  - Metric
  - Reported takeaway
  rows:
  - - Darcy/Burgers
    - L2 error
    - Competitive accuracy with attention that scales better than full self-attention.
status: curated
method_class: Transformers
auto:
  pdes: []
  tasks: []
