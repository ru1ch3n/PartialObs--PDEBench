<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Galerkin Transformer (2021) — Research — PartialObs-PDEBench</title>
  <link rel="stylesheet" href="../../assets/style.css" />
  <script>window.MathJax={tex:{inlineMath:[['\(','\)'],['$','$']]}};</script><script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script defer src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script><script>document.addEventListener('DOMContentLoaded',function(){if(window.mermaid){mermaid.initialize({startOnLoad:true,theme:'dark'});}});</script>
</head>

<body>
  <header class="hero">
    <div class="container">
      <div class="hero-top">
        <div>
          <h1>Galerkin Transformer (2021)</h1>
          <p class="subtitle"><b>Choose a Transformer: Fourier or Galerkin</b><br/>Shanghong Cao</p>
          <div class="meta"><div><b>Paper:</b> <a class="meta-link" href="https://arxiv.org/abs/2105.14995" target="_blank" rel="noopener noreferrer">arXiv:2105.14995</a></div></div><div class="badges">
<span class="badge">Transformer</span>
<span class="badge">Operator learning</span>
</div>
        </div>

        <div class="hero-card">  <div class="smallcaps">Quick facts</div>  <p class="muted" style="margin-top:8px;">Type: attention-based operator<br/>Fourier/Galerkin attention<br/>Mesh-independent flavor</p>  <p style="margin:10px 0 0;"><a href="../index.html">← Research</a> · <a href="../../index.html">Home</a></p></div>
      </div>

      <nav class="nav"><a href="../../index.html">Home</a><a href="../../research/" aria-current="page">Research</a><a href="../../pde-problems/">PDE problems</a><a href="../../baselines/">Baselines</a><a href="../../benchmark/">Benchmark</a></nav>
    </div>
  </header>

  <main class="container">
    <section id="tldr"><h2>TL;DR</h2><p>This work adapts transformer attention to operator learning with Fourier or Galerkin-inspired variants, aiming for efficient and stable PDE surrogate modeling.</p></section>
<section id="core-math"><h2>Core method (math)</h2><p class="muted">Template for <b>Transformers</b>. Paper-specific equations are added when manually curated.</p><div class="equation">\[\mathrm{Attn}(Q,K,V)=\mathrm{softmax}(QK^\top/\sqrt{d})V\ \ \text{(global token mixing)}\]</div></section>
<section id="theory"><h2>Main theoretical contribution</h2><p class="muted">Not extracted yet (paper page is index-only, or theory not summarized).</p></section>
<section id="contribution"><h2>Main contribution</h2><ul>
<li>Introduces Fourier and Galerkin attention mechanisms inspired by numerical solvers.</li>
<li>Demonstrates operator learning on benchmark PDEs.</li>
<li>Shows efficiency advantages over naive attention for high-resolution fields.</li>
</ul></section>
<section id="experiments"><h2>Experiments</h2><div class="grid2">  <div class="card"><h3>PDE problems (as reported)</h3><ul>
<li>Burgers</li>
<li>Darcy flow</li>
<li>Inverse interface coefficient problem</li>
</ul></div>  <div class="card"><h3>Tasks</h3><ul>
<li>Forward operator learning</li>
<li>Inverse problem inference</li>
</ul></div></div><div class="card" style="margin-top:14px;"><h3>Experiment setting (high level)</h3><ul>
<li>Supervised operator learning; compares attention variants.</li>
<li>Evaluates on standard PDE datasets and inverse coefficient estimation.</li>
</ul></div></section>
<section id="baselines"><h2>Comparable baselines</h2><ul>
<li>FNO</li>
<li>DeepONet</li>
<li>U-Net</li>
</ul></section>
<section id="results"><h2>Main results</h2>
<h3 class="subhead">Key results</h3>
<div class="tablewrap"><table><thead><tr><th>Benchmark</th><th>Metric</th><th>Reported takeaway</th></tr></thead><tbody><tr><td>Darcy/Burgers</td><td>L2 error</td><td>Competitive accuracy with attention that scales better than full self-attention.</td></tr></tbody></table></div>
</section>

    <footer class="footer">
      <div class="muted">Last generated: 2026-01-20</div>
    </footer>
  </main>
</body>
</html>
